self.stack = ['ROOT']
self.buffer = sentence
self.dependencies = []

if transition == "S":
  self.stack.append(self.buffer[0])
  self.buffer = self.buffer[1:]
if transition == "LA":
  self.dependencies.append((self.stack[-1],self.stack[-2]))
  self.stack.pop(-2)
if transition == "RA":
  self.dependencies.append((self.stack[-2],self.stack[-1]))
  self.stack.pop(-1)
  

partial_parses = [PartialParse(sentence) for sentence in sentences]
unfinished_parses = partial_parses[:]
pbar = tqdm(total=len(unfinished_parses),desc='Parsing')
while len(unfinished_parses) > 0:
    mini_batch = unfinished_parses[:batch_size]
    for parser,parser_next_step in zip(mini_batch,model.predict(mini_batch)):
        parser.parse_step(parser_next_step)
        if parser.is_parsed():
            unfinished_parses.remove(parser)
            pbar.update(1)
dependencies = [parser.dependencies for parser in partial_parses]


## Pytorch - nn 
self.embed_to_hidden = torch.nn.Linear(embed_size*self.n_features,hidden_size)
self.embed_to_hidden.weight = nn.init.xavier_uniform_(self.embed_to_hidden.weight)
self.dropout = nn.Dropout(p=dropout_prob)
self.hidden_to_logits = torch.nn.Linear(hidden_size, n_classes, bias=True)
self.hidden_to_logits.weight = nn.init.xavier_uniform_(self.hidden_to_logits.weight)

x_embed = self.pretrained_embeddings(t)
x = x_embed.view(t.shape[0],-1)

embed_layer = self.embedding_lookup(t)
h1 = self.embed_to_hidden(embed_layer)
h1 = nn.functional.relu(h1)
do = self.dropout(h1)
logits = self.hidden_to_logits(do)

optimizer = torch.optim.Adam(parser.model.parameters(),lr=lr)
loss_func = nn.CrossEntropyLoss()  


logits = parser.model(train_x)
loss = loss_func(logits,train_y)
# Backward pass  Auto differntiation not much of an worry
loss.backward()
#Update using optimizer after calculating loss
optimizer.step()



## Assignment  - 4

max_len = np.max(np.array([len(sent) for sent in sample_l]))

for sent in sample_l:
  
  if len(sent) != max_len:
    
    while len(sent) < max_len:
      sent.append(pad_token)
          
  else:
     pass
