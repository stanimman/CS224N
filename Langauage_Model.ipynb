{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled13.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stanimman/CS224N/blob/master/Langauage_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlWGCclyI3m-",
        "colab_type": "code",
        "outputId": "e03b4122-a338-40f9-dda9-69971b4ff96f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "!git clone https://github.com/ceshine/examples.git pytorch_examples"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'pytorch_examples'...\n",
            "remote: Enumerating objects: 1725, done.\u001b[K\n",
            "remote: Total 1725 (delta 0), reused 0 (delta 0), pack-reused 1725\n",
            "Receiving objects: 100% (1725/1725), 38.85 MiB | 17.13 MiB/s, done.\n",
            "Resolving deltas: 100% (905/905), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IW5KNvf9JRMW",
        "colab_type": "code",
        "outputId": "8c73fc3a-36a9-4096-e73d-b76af20671e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "%cd pytorch_examples/word_language_model\n",
        "%ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/pytorch_examples/word_language_model\n",
            "\u001b[0m\u001b[01;34mdata\u001b[0m/    generate.py  model.py    README.md\n",
            "data.py  main.py      \u001b[01;34mnotebooks\u001b[0m/  requirements.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sR56ZXrNJZbX",
        "colab_type": "code",
        "outputId": "6c04c461-57f3-4408-fb69-57cf8a9ae432",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Jun 16 10:48:58 2019       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 418.67       Driver Version: 410.79       CUDA Version: 10.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   55C    P8    16W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGmacBfDJquc",
        "colab_type": "code",
        "outputId": "b0e46d89-694d-4e0a-e9f9-31609438189d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        }
      },
      "source": [
        "%%time \n",
        "!python -u main.py --cuda --emsize 650 --nhid 650 --dropout 0.5 --epochs 2 --tied  2>&1 | tee train.log  # Test perplexity of 75.96"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of tokens:\n",
            "Train:  2075677\n",
            "Valid:  216347\n",
            "Test:   244102\n",
            "| epoch   1 |   200/ 2965 batches | lr 20.00 | ms/batch 52.02 | loss  7.75 | ppl  2313.00\n",
            "| epoch   1 |   400/ 2965 batches | lr 20.00 | ms/batch 51.73 | loss  7.09 | ppl  1201.09\n",
            "| epoch   1 |   600/ 2965 batches | lr 20.00 | ms/batch 52.43 | loss  6.47 | ppl   643.67\n",
            "| epoch   1 |   800/ 2965 batches | lr 20.00 | ms/batch 52.96 | loss  6.29 | ppl   537.78\n",
            "| epoch   1 |  1000/ 2965 batches | lr 20.00 | ms/batch 53.80 | loss  6.16 | ppl   473.04\n",
            "| epoch   1 |  1200/ 2965 batches | lr 20.00 | ms/batch 54.49 | loss  6.08 | ppl   436.73\n",
            "| epoch   1 |  1400/ 2965 batches | lr 20.00 | ms/batch 54.97 | loss  5.96 | ppl   388.92\n",
            "| epoch   1 |  1600/ 2965 batches | lr 20.00 | ms/batch 55.37 | loss  5.97 | ppl   390.90\n",
            "| epoch   1 |  1800/ 2965 batches | lr 20.00 | ms/batch 55.81 | loss  5.82 | ppl   336.88\n",
            "| epoch   1 |  2000/ 2965 batches | lr 20.00 | ms/batch 56.10 | loss  5.77 | ppl   320.18\n",
            "| epoch   1 |  2200/ 2965 batches | lr 20.00 | ms/batch 56.54 | loss  5.67 | ppl   289.38\n",
            "| epoch   1 |  2400/ 2965 batches | lr 20.00 | ms/batch 57.09 | loss  5.69 | ppl   296.08\n",
            "| epoch   1 |  2600/ 2965 batches | lr 20.00 | ms/batch 57.32 | loss  5.68 | ppl   291.99\n",
            "| epoch   1 |  2800/ 2965 batches | lr 20.00 | ms/batch 57.24 | loss  5.55 | ppl   256.92\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 170.77s | valid loss  5.45 | valid ppl   232.47\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   2 |   200/ 2965 batches | lr 20.00 | ms/batch 56.87 | loss  5.57 | ppl   261.91\n",
            "| epoch   2 |   400/ 2965 batches | lr 20.00 | ms/batch 56.54 | loss  5.56 | ppl   260.08\n",
            "| epoch   2 |   600/ 2965 batches | lr 20.00 | ms/batch 56.69 | loss  5.38 | ppl   217.40\n",
            "| epoch   2 |   800/ 2965 batches | lr 20.00 | ms/batch 56.83 | loss  5.40 | ppl   221.39\n",
            "| epoch   2 |  1000/ 2965 batches | lr 20.00 | ms/batch 56.95 | loss  5.36 | ppl   213.56\n",
            "| epoch   2 |  1200/ 2965 batches | lr 20.00 | ms/batch 57.05 | loss  5.36 | ppl   212.73\n",
            "| epoch   2 |  1400/ 2965 batches | lr 20.00 | ms/batch 57.08 | loss  5.34 | ppl   209.45\n",
            "| epoch   2 |  1600/ 2965 batches | lr 20.00 | ms/batch 57.07 | loss  5.41 | ppl   222.82\n",
            "| epoch   2 |  1800/ 2965 batches | lr 20.00 | ms/batch 57.05 | loss  5.28 | ppl   196.87\n",
            "| epoch   2 |  2000/ 2965 batches | lr 20.00 | ms/batch 56.99 | loss  5.28 | ppl   196.98\n",
            "| epoch   2 |  2200/ 2965 batches | lr 20.00 | ms/batch 56.87 | loss  5.19 | ppl   179.32\n",
            "| epoch   2 |  2400/ 2965 batches | lr 20.00 | ms/batch 56.84 | loss  5.23 | ppl   187.04\n",
            "| epoch   2 |  2600/ 2965 batches | lr 20.00 | ms/batch 56.85 | loss  5.24 | ppl   189.46\n",
            "| epoch   2 |  2800/ 2965 batches | lr 20.00 | ms/batch 56.77 | loss  5.14 | ppl   170.97\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 176.44s | valid loss  5.14 | valid ppl   170.28\n",
            "-----------------------------------------------------------------------------------------\n",
            "=========================================================================================\n",
            "| End of training | test loss  5.07 | test ppl   158.42\n",
            "=========================================================================================\n",
            "CPU times: user 1.28 s, sys: 161 ms, total: 1.44 s\n",
            "Wall time: 6min 31s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HFrP3dp674K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7d111f4f-cc1b-48c4-b16e-86a0e598c41c"
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# 2. Save Keras Model or weights on google drive\n",
        "\n",
        "# create on Colab directory\n",
        "#model.save('model.pt')    \n",
        "model_file = drive.CreateFile({'title' : 'model.pt'})\n",
        "model_file.SetContentFile('model.pt')\n",
        "model_file.Upload()\n",
        "\n",
        "# download to google drive\n",
        "drive.CreateFile({'id': model_file.get('id')})"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GoogleDriveFile({'id': '1VI5kShmA_cInIBkVvsaeSI4-TdH-oIeJ'})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifBjMyzeP72-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqB8TNwjA27M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "c09a658c-7f6b-4580-e05b-c215833e3f1b"
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtD2h1PGBDLO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "outputId": "afd58764-bdfb-4498-9f2d-4bad9315d527"
      },
      "source": [
        "!ls \"/content/drive/My Drive\""
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'02_ Lecture note_ TensorFlow Ops.gdoc'\n",
            "'2013 (1).xlsx.gsheet'\n",
            " 2013.xlsx\n",
            " 2013.xlsx.gsheet\n",
            "'201501 - Wealth Advisors - Insights.pdf'\n",
            " 2018-02-08-PHOTO-00013482.jpg\n",
            " 2020024.pdf\n",
            "'Analyst Job 4.docx'\n",
            "'Coconut Punch Pouch.jpg'\n",
            "'COLAB DATASET'\n",
            "'Colab Notebooks'\n",
            " Comments.xlsx\n",
            " Comments.xlsx.gsheet\n",
            "'Copy of Data science training resources.gsheet'\n",
            " cs229-notes1.pdf\n",
            " DSC_0847.jpg\n",
            " export.pdf\n",
            "'Fruitz Orange 696g Label.jpg'\n",
            "'hudson taylor.gdoc'\n",
            " Important_Docx\n",
            " jfdbbdcnbbkmggeo.png\n",
            " June_Statement.pdf\n",
            " Kingsley_BioData.doc\n",
            "'LatentView Senior Analyst Job Description (1).pdf'\n",
            "'License Scan.docx'\n",
            "'local conveyance_banglore.gsheet'\n",
            " Lotte\n",
            " Marriage_Photos\n",
            " model.pt\n",
            " MyAccount.pdf\n",
            "'my cry on 23 1 2010(The day after my ....gdoc'\n",
            "'My Saved Places.gmap'\n",
            " spanis-king.jpg\n",
            " Stanley_Resume.docx\n",
            " Stanley_Resume.docx.gdoc\n",
            "'TNEB Online Payment.pdf'\n",
            "'TN - GIM'\n",
            "'Untitled document (1).gdoc'\n",
            "'Untitled document.gdoc'\n",
            "'Untitled presentation.gslides'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWSfru43UnBM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp \"/content/drive/My Drive/model.pt\" \"model.pt\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "la-Fz34TBX95",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "a52eb0c3-df2a-4fe0-b221-085171d3c4ff"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " data\t       main.py\t       model.py      README.md\n",
            " data.py      'model (1).pt'   notebooks     requirements.txt\n",
            " generate.py   model.pt        __pycache__   train.log\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bY-zuFzyB8sI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from model import RNNModel\n",
        "from data import Dictionary, Corpus"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_IiIr5_CCzl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "a03e5c8b-1dd3-4cfc-8c61-fd6381e6ffed"
      },
      "source": [
        "DATA_PATH = \"./data/wikitext-2\"\n",
        "corpus = Corpus(DATA_PATH)\n",
        "\n",
        "print(\"Number of tokens:\")\n",
        "print(\"Train: \", len(corpus.train))\n",
        "print(\"Valid: \", len(corpus.valid))\n",
        "print(\"Test:  \", len(corpus.test))\n",
        "\n",
        "print(\"Vocabulary size:\", len(corpus.dictionary.idx2word))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of tokens:\n",
            "Train:  2075677\n",
            "Valid:  216347\n",
            "Test:   244102\n",
            "Vocabulary size: 33278\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPUj_wkeCF1z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "82fd6540-b751-42bc-d47f-48479077d519"
      },
      "source": [
        "# setting device on GPU if available, else CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "print()\n",
        "\n",
        "#Additional Info when using cuda\n",
        "if device.type == 'cuda':\n",
        "    print(torch.cuda.get_device_name(0))\n",
        "    print('Memory Usage:')\n",
        "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
        "    print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "\n",
            "Tesla T4\n",
            "Memory Usage:\n",
            "Allocated: 0.0 GB\n",
            "Cached:    0.0 GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6nzs-RRBYpN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"/content/drive/My Drive/model.pt\", 'rb') as f:\n",
        "    model = torch.load(f, map_location=device)\n",
        "model = model.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGHz-6IXCnjh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "ef357cc3-2123-4e09-8110-0257464d9262"
      },
      "source": [
        "model.eval()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RNNModel(\n",
              "  (drop): Dropout(p=0.5)\n",
              "  (encoder): Embedding(33278, 650)\n",
              "  (rnn): LSTM(650, 650, num_layers=2, dropout=0.5)\n",
              "  (decoder): Linear(in_features=650, out_features=33278, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bPG5ovzDAix",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "17c12924-bfd1-4992-b725-8103558c3e58"
      },
      "source": [
        "%%time\n",
        "BPTT = 50\n",
        "CRITERION = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "def batchify(data, bsz):\n",
        "    # Work out how cleanly we can divide the dataset into bsz parts.\n",
        "    nbatch = data.size(0) // bsz\n",
        "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
        "    data = data.narrow(0, 0, nbatch * bsz)\n",
        "    # Evenly divide the data across the bsz batches.\n",
        "    data = data.view(bsz, -1).t().contiguous()\n",
        "    return data.to(device)\n",
        "\n",
        "def get_batch(source, i):\n",
        "    seq_len = min(BPTT, len(source) - 1 - i)\n",
        "    data = source[i:i+seq_len]\n",
        "    target = source[i+1:i+1+seq_len].view(-1)\n",
        "    return data, target\n",
        "\n",
        "def evaluate(data_source):\n",
        "    # Turn on evaluation mode which disables dropout.\n",
        "    model.eval()\n",
        "    total_loss = 0.\n",
        "    ntokens = len(corpus.dictionary)\n",
        "    hidden = model.init_hidden(10)\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, data_source.size(0) - 1, BPTT):\n",
        "            data, targets = get_batch(data_source, i)\n",
        "            output, hidden = model(data, hidden)\n",
        "            output_flat = output.view(-1, ntokens)\n",
        "            total_loss += len(data) * CRITERION(output_flat, targets).item()\n",
        "            hidden = repackage_hidden(hidden)\n",
        "    return total_loss / len(data_source)\n",
        "\n",
        "def repackage_hidden(h):\n",
        "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
        "    if isinstance(h, torch.Tensor):\n",
        "        return h.detach()\n",
        "    else:\n",
        "        return tuple(repackage_hidden(v) for v in h)\n",
        "    \n",
        "test_data = batchify(corpus.test, 10)\n",
        "loss = evaluate(test_data)\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 4.61 s, sys: 3.08 s, total: 7.7 s\n",
            "Wall time: 7.72 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NslQJAVbDMIl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "29aaf094-30c4-48de-aa73-90090269fbdb"
      },
      "source": [
        "loss, np.exp(loss)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5.065246568357099, 158.4194984939212)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRtSWmMwDMoE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7c6f26fa-43bd-4f3d-c40c-56fd097a854a"
      },
      "source": [
        "test_tokens = corpus.test.numpy()\n",
        "eos_pos = np.where(test_tokens == corpus.dictionary.word2idx[\"<eos>\"])[0]\n",
        "print(\"Number of lines in test:\", len(eos_pos))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of lines in test: 2891\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LiJkIM_DaKY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "3e30e978-9f06-4509-ce75-5c8343e6df3f"
      },
      "source": [
        "# A random line from test dataset\n",
        "print(\" \".join([corpus.dictionary.idx2word[c] for c in test_tokens[eos_pos[28]+1:eos_pos[29]]]))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The An <unk> Rebellion began in December <unk> , and was not completely suppressed for almost eight years . It caused enormous disruption to Chinese society : the census of 754 recorded 52 @.@ 9 million people , but ten years later , the census counted just 16 @.@ 9 million , the remainder having been displaced or killed . During this time , Du Fu led a largely itinerant life <unk> by wars , associated <unk> and imperial <unk> . This period of <unk> was the making of Du Fu as a poet : Even Shan Chou has written that , \" What he saw around him — the lives of his family , neighbors , and strangers – what he heard , and what he hoped for or feared from the progress of various campaigns — these became the enduring themes of his poetry \" . Even when he learned of the death of his youngest child , he turned to the suffering of others in his poetry instead of dwelling upon his own <unk> . Du Fu wrote :\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0YXXGvADd2N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def eval_chunk(start, end):\n",
        "    token_tensor = corpus.test[eos_pos[start]+1:eos_pos[end]]\n",
        "    hidden = model.init_hidden(1)\n",
        "    with torch.no_grad():\n",
        "        targets = token_tensor[1:].to(device)\n",
        "        output, hidden = model(token_tensor.to(device).unsqueeze(1), hidden)\n",
        "        output_flat = output.squeeze(1)\n",
        "        loss = CRITERION(output_flat[:-1], targets).item()\n",
        "    \n",
        "    sorted_idx = np.argsort(output_flat.cpu().numpy(), 1)\n",
        "    #sorted_idx = sorted_idx.to(device)\n",
        "    preds = []\n",
        "    for i in range(1, 4):\n",
        "        preds.append(list(map(lambda x: corpus.dictionary.idx2word[x], sorted_idx[:, -i])))\n",
        "    # preds = list(map(lambda x: itos[x], np.argmax(logits.data.cpu().numpy(), 1)))\n",
        "    return (\n",
        "        loss,\n",
        "        pd.DataFrame({\n",
        "            \"orig\": [corpus.dictionary.idx2word[x] for x in token_tensor.numpy()] + [\" \"], \n",
        "            \"pred_1\": [\"\"] + preds[0], \"pred_2\": [\"\"] + preds[1], \"pred_3\": [\"\"] + preds[2]\n",
        "        })\n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xs1-nBqoDfSp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1616
        },
        "outputId": "d778be27-6cc1-48fc-b11c-780e77958c80"
      },
      "source": [
        "loss, df = eval_chunk(28, 29)\n",
        "print(\"Loss:\", np.exp(loss))\n",
        "df.iloc[-50:]"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss: 241.3071571373958\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>orig</th>\n",
              "      <th>pred_1</th>\n",
              "      <th>pred_2</th>\n",
              "      <th>pred_3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>133</th>\n",
              "      <td>progress</td>\n",
              "      <td>&lt;unk&gt;</td>\n",
              "      <td>same</td>\n",
              "      <td>other</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>134</th>\n",
              "      <td>of</td>\n",
              "      <td>of</td>\n",
              "      <td>.</td>\n",
              "      <td>\"</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>135</th>\n",
              "      <td>various</td>\n",
              "      <td>the</td>\n",
              "      <td>a</td>\n",
              "      <td>his</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>136</th>\n",
              "      <td>campaigns</td>\n",
              "      <td>.</td>\n",
              "      <td>people</td>\n",
              "      <td>or</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>137</th>\n",
              "      <td>—</td>\n",
              "      <td>.</td>\n",
              "      <td>and</td>\n",
              "      <td>,</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>138</th>\n",
              "      <td>these</td>\n",
              "      <td>and</td>\n",
              "      <td>the</td>\n",
              "      <td>it</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>139</th>\n",
              "      <td>became</td>\n",
              "      <td>are</td>\n",
              "      <td>is</td>\n",
              "      <td>were</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>140</th>\n",
              "      <td>the</td>\n",
              "      <td>a</td>\n",
              "      <td>the</td>\n",
              "      <td>&lt;unk&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>141</th>\n",
              "      <td>enduring</td>\n",
              "      <td>most</td>\n",
              "      <td>same</td>\n",
              "      <td>only</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142</th>\n",
              "      <td>themes</td>\n",
              "      <td>and</td>\n",
              "      <td>&lt;unk&gt;</td>\n",
              "      <td>\"</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>143</th>\n",
              "      <td>of</td>\n",
              "      <td>of</td>\n",
              "      <td>in</td>\n",
              "      <td>.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>144</th>\n",
              "      <td>his</td>\n",
              "      <td>the</td>\n",
              "      <td>a</td>\n",
              "      <td>this</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145</th>\n",
              "      <td>poetry</td>\n",
              "      <td>own</td>\n",
              "      <td>life</td>\n",
              "      <td>\"</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>146</th>\n",
              "      <td>\"</td>\n",
              "      <td>.</td>\n",
              "      <td>\"</td>\n",
              "      <td>and</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>147</th>\n",
              "      <td>.</td>\n",
              "      <td>.</td>\n",
              "      <td>,</td>\n",
              "      <td>and</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>148</th>\n",
              "      <td>Even</td>\n",
              "      <td>&lt;eos&gt;</td>\n",
              "      <td>The</td>\n",
              "      <td>In</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149</th>\n",
              "      <td>when</td>\n",
              "      <td>the</td>\n",
              "      <td>that</td>\n",
              "      <td>of</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>150</th>\n",
              "      <td>he</td>\n",
              "      <td>the</td>\n",
              "      <td>he</td>\n",
              "      <td>it</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>151</th>\n",
              "      <td>learned</td>\n",
              "      <td>was</td>\n",
              "      <td>is</td>\n",
              "      <td>had</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>152</th>\n",
              "      <td>of</td>\n",
              "      <td>the</td>\n",
              "      <td>to</td>\n",
              "      <td>,</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>153</th>\n",
              "      <td>the</td>\n",
              "      <td>the</td>\n",
              "      <td>his</td>\n",
              "      <td>a</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>154</th>\n",
              "      <td>death</td>\n",
              "      <td>&lt;unk&gt;</td>\n",
              "      <td>\"</td>\n",
              "      <td>story</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>155</th>\n",
              "      <td>of</td>\n",
              "      <td>of</td>\n",
              "      <td>,</td>\n",
              "      <td>and</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>156</th>\n",
              "      <td>his</td>\n",
              "      <td>the</td>\n",
              "      <td>his</td>\n",
              "      <td>a</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>157</th>\n",
              "      <td>youngest</td>\n",
              "      <td>father</td>\n",
              "      <td>life</td>\n",
              "      <td>death</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>158</th>\n",
              "      <td>child</td>\n",
              "      <td>son</td>\n",
              "      <td>wife</td>\n",
              "      <td>year</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159</th>\n",
              "      <td>,</td>\n",
              "      <td>,</td>\n",
              "      <td>in</td>\n",
              "      <td>and</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>160</th>\n",
              "      <td>he</td>\n",
              "      <td>the</td>\n",
              "      <td>he</td>\n",
              "      <td>it</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>161</th>\n",
              "      <td>turned</td>\n",
              "      <td>was</td>\n",
              "      <td>is</td>\n",
              "      <td>had</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>162</th>\n",
              "      <td>to</td>\n",
              "      <td>to</td>\n",
              "      <td>into</td>\n",
              "      <td>up</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>163</th>\n",
              "      <td>the</td>\n",
              "      <td>the</td>\n",
              "      <td>a</td>\n",
              "      <td>his</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>164</th>\n",
              "      <td>suffering</td>\n",
              "      <td>&lt;unk&gt;</td>\n",
              "      <td>city</td>\n",
              "      <td>race</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>165</th>\n",
              "      <td>of</td>\n",
              "      <td>of</td>\n",
              "      <td>,</td>\n",
              "      <td>@-@</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>166</th>\n",
              "      <td>others</td>\n",
              "      <td>the</td>\n",
              "      <td>his</td>\n",
              "      <td>a</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>167</th>\n",
              "      <td>in</td>\n",
              "      <td>.</td>\n",
              "      <td>,</td>\n",
              "      <td>and</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>168</th>\n",
              "      <td>his</td>\n",
              "      <td>the</td>\n",
              "      <td>his</td>\n",
              "      <td>a</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>169</th>\n",
              "      <td>poetry</td>\n",
              "      <td>own</td>\n",
              "      <td>first</td>\n",
              "      <td>life</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>170</th>\n",
              "      <td>instead</td>\n",
              "      <td>.</td>\n",
              "      <td>,</td>\n",
              "      <td>and</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>171</th>\n",
              "      <td>of</td>\n",
              "      <td>.</td>\n",
              "      <td>of</td>\n",
              "      <td>,</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>172</th>\n",
              "      <td>dwelling</td>\n",
              "      <td>the</td>\n",
              "      <td>his</td>\n",
              "      <td>a</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>173</th>\n",
              "      <td>upon</td>\n",
              "      <td>.</td>\n",
              "      <td>,</td>\n",
              "      <td>and</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>174</th>\n",
              "      <td>his</td>\n",
              "      <td>the</td>\n",
              "      <td>a</td>\n",
              "      <td>his</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>175</th>\n",
              "      <td>own</td>\n",
              "      <td>death</td>\n",
              "      <td>father</td>\n",
              "      <td>own</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>176</th>\n",
              "      <td>&lt;unk&gt;</td>\n",
              "      <td>.</td>\n",
              "      <td>son</td>\n",
              "      <td>death</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>177</th>\n",
              "      <td>.</td>\n",
              "      <td>.</td>\n",
              "      <td>,</td>\n",
              "      <td>and</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>178</th>\n",
              "      <td>Du</td>\n",
              "      <td>The</td>\n",
              "      <td>&lt;eos&gt;</td>\n",
              "      <td>In</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>179</th>\n",
              "      <td>Fu</td>\n",
              "      <td>&lt;unk&gt;</td>\n",
              "      <td>and</td>\n",
              "      <td>,</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>180</th>\n",
              "      <td>wrote</td>\n",
              "      <td>was</td>\n",
              "      <td>,</td>\n",
              "      <td>of</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>181</th>\n",
              "      <td>:</td>\n",
              "      <td>that</td>\n",
              "      <td>the</td>\n",
              "      <td>a</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>182</th>\n",
              "      <td></td>\n",
              "      <td>\"</td>\n",
              "      <td>&lt;eos&gt;</td>\n",
              "      <td>The</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          orig  pred_1  pred_2 pred_3\n",
              "133   progress   <unk>    same  other\n",
              "134         of      of       .      \"\n",
              "135    various     the       a    his\n",
              "136  campaigns       .  people     or\n",
              "137          —       .     and      ,\n",
              "138      these     and     the     it\n",
              "139     became     are      is   were\n",
              "140        the       a     the  <unk>\n",
              "141   enduring    most    same   only\n",
              "142     themes     and   <unk>      \"\n",
              "143         of      of      in      .\n",
              "144        his     the       a   this\n",
              "145     poetry     own    life      \"\n",
              "146          \"       .       \"    and\n",
              "147          .       .       ,    and\n",
              "148       Even   <eos>     The     In\n",
              "149       when     the    that     of\n",
              "150         he     the      he     it\n",
              "151    learned     was      is    had\n",
              "152         of     the      to      ,\n",
              "153        the     the     his      a\n",
              "154      death   <unk>       \"  story\n",
              "155         of      of       ,    and\n",
              "156        his     the     his      a\n",
              "157   youngest  father    life  death\n",
              "158      child     son    wife   year\n",
              "159          ,       ,      in    and\n",
              "160         he     the      he     it\n",
              "161     turned     was      is    had\n",
              "162         to      to    into     up\n",
              "163        the     the       a    his\n",
              "164  suffering   <unk>    city   race\n",
              "165         of      of       ,    @-@\n",
              "166     others     the     his      a\n",
              "167         in       .       ,    and\n",
              "168        his     the     his      a\n",
              "169     poetry     own   first   life\n",
              "170    instead       .       ,    and\n",
              "171         of       .      of      ,\n",
              "172   dwelling     the     his      a\n",
              "173       upon       .       ,    and\n",
              "174        his     the       a    his\n",
              "175        own   death  father    own\n",
              "176      <unk>       .     son  death\n",
              "177          .       .       ,    and\n",
              "178         Du     The   <eos>     In\n",
              "179         Fu   <unk>     and      ,\n",
              "180      wrote     was       ,     of\n",
              "181          :    that     the      a\n",
              "182                  \"   <eos>    The"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4kOJpasG8-I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1616
        },
        "outputId": "d834016d-86ba-412b-e9f0-870820086b38"
      },
      "source": [
        "loss, df = eval_chunk(28, 34)\n",
        "print(\"Loss:\", np.exp(loss))\n",
        "df.iloc[-50:]"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss: 162.85196943697997\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>orig</th>\n",
              "      <th>pred_1</th>\n",
              "      <th>pred_2</th>\n",
              "      <th>pred_3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>489</th>\n",
              "      <td>in</td>\n",
              "      <td>a</td>\n",
              "      <td>to</td>\n",
              "      <td>the</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>490</th>\n",
              "      <td>the</td>\n",
              "      <td>the</td>\n",
              "      <td>a</td>\n",
              "      <td>&lt;unk&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>491</th>\n",
              "      <td>summer</td>\n",
              "      <td>&lt;unk&gt;</td>\n",
              "      <td>early</td>\n",
              "      <td>late</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>492</th>\n",
              "      <td>of</td>\n",
              "      <td>of</td>\n",
              "      <td>,</td>\n",
              "      <td>and</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>493</th>\n",
              "      <td>&lt;unk&gt;</td>\n",
              "      <td>the</td>\n",
              "      <td>&lt;unk&gt;</td>\n",
              "      <td>December</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>494</th>\n",
              "      <td>;</td>\n",
              "      <td>,</td>\n",
              "      <td>and</td>\n",
              "      <td>.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>495</th>\n",
              "      <td>this</td>\n",
              "      <td>the</td>\n",
              "      <td>in</td>\n",
              "      <td>he</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>496</th>\n",
              "      <td>has</td>\n",
              "      <td>was</td>\n",
              "      <td>is</td>\n",
              "      <td>time</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>497</th>\n",
              "      <td>traditionally</td>\n",
              "      <td>been</td>\n",
              "      <td>no</td>\n",
              "      <td>also</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>498</th>\n",
              "      <td>been</td>\n",
              "      <td>been</td>\n",
              "      <td>died</td>\n",
              "      <td>suggested</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>499</th>\n",
              "      <td>ascribed</td>\n",
              "      <td>killed</td>\n",
              "      <td>known</td>\n",
              "      <td>the</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>500</th>\n",
              "      <td>to</td>\n",
              "      <td>to</td>\n",
              "      <td>in</td>\n",
              "      <td>by</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>501</th>\n",
              "      <td>famine</td>\n",
              "      <td>the</td>\n",
              "      <td>be</td>\n",
              "      <td>have</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>502</th>\n",
              "      <td>,</td>\n",
              "      <td>.</td>\n",
              "      <td>,</td>\n",
              "      <td>in</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>503</th>\n",
              "      <td>but</td>\n",
              "      <td>and</td>\n",
              "      <td>but</td>\n",
              "      <td>with</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>504</th>\n",
              "      <td>&lt;unk&gt;</td>\n",
              "      <td>the</td>\n",
              "      <td>that</td>\n",
              "      <td>it</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>505</th>\n",
              "      <td>believes</td>\n",
              "      <td>&lt;unk&gt;</td>\n",
              "      <td>the</td>\n",
              "      <td>,</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>506</th>\n",
              "      <td>that</td>\n",
              "      <td>that</td>\n",
              "      <td>the</td>\n",
              "      <td>it</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>507</th>\n",
              "      <td>frustration</td>\n",
              "      <td>the</td>\n",
              "      <td>it</td>\n",
              "      <td>he</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>508</th>\n",
              "      <td>is</td>\n",
              "      <td>were</td>\n",
              "      <td>was</td>\n",
              "      <td>is</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>509</th>\n",
              "      <td>a</td>\n",
              "      <td>\"</td>\n",
              "      <td>not</td>\n",
              "      <td>the</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>510</th>\n",
              "      <td>more</td>\n",
              "      <td>\"</td>\n",
              "      <td>&lt;unk&gt;</td>\n",
              "      <td>good</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>511</th>\n",
              "      <td>likely</td>\n",
              "      <td>&lt;unk&gt;</td>\n",
              "      <td>powerful</td>\n",
              "      <td>\"</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>512</th>\n",
              "      <td>reason</td>\n",
              "      <td>to</td>\n",
              "      <td>that</td>\n",
              "      <td>of</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>513</th>\n",
              "      <td>.</td>\n",
              "      <td>that</td>\n",
              "      <td>to</td>\n",
              "      <td>.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>514</th>\n",
              "      <td>He</td>\n",
              "      <td>&lt;eos&gt;</td>\n",
              "      <td>The</td>\n",
              "      <td>In</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>515</th>\n",
              "      <td>next</td>\n",
              "      <td>was</td>\n",
              "      <td>is</td>\n",
              "      <td>also</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>516</th>\n",
              "      <td>spent</td>\n",
              "      <td>the</td>\n",
              "      <td>&lt;unk&gt;</td>\n",
              "      <td>not</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>517</th>\n",
              "      <td>around</td>\n",
              "      <td>a</td>\n",
              "      <td>the</td>\n",
              "      <td>his</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>518</th>\n",
              "      <td>six</td>\n",
              "      <td>the</td>\n",
              "      <td>his</td>\n",
              "      <td>a</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>519</th>\n",
              "      <td>weeks</td>\n",
              "      <td>years</td>\n",
              "      <td>months</td>\n",
              "      <td>days</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>520</th>\n",
              "      <td>in</td>\n",
              "      <td>,</td>\n",
              "      <td>of</td>\n",
              "      <td>in</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>521</th>\n",
              "      <td>&lt;unk&gt;</td>\n",
              "      <td>the</td>\n",
              "      <td>&lt;unk&gt;</td>\n",
              "      <td>a</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>522</th>\n",
              "      <td>(</td>\n",
              "      <td>,</td>\n",
              "      <td>.</td>\n",
              "      <td>and</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>523</th>\n",
              "      <td>now</td>\n",
              "      <td>died</td>\n",
              "      <td>&lt;unk&gt;</td>\n",
              "      <td>the</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>524</th>\n",
              "      <td>&lt;unk&gt;</td>\n",
              "      <td>&lt;unk&gt;</td>\n",
              "      <td>the</td>\n",
              "      <td>known</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>525</th>\n",
              "      <td>,</td>\n",
              "      <td>)</td>\n",
              "      <td>,</td>\n",
              "      <td>&lt;unk&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>526</th>\n",
              "      <td>Gansu</td>\n",
              "      <td>&lt;unk&gt;</td>\n",
              "      <td>and</td>\n",
              "      <td>the</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>527</th>\n",
              "      <td>province</td>\n",
              "      <td>)</td>\n",
              "      <td>,</td>\n",
              "      <td>and</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>528</th>\n",
              "      <td>)</td>\n",
              "      <td>)</td>\n",
              "      <td>,</td>\n",
              "      <td>and</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>529</th>\n",
              "      <td>,</td>\n",
              "      <td>,</td>\n",
              "      <td>.</td>\n",
              "      <td>and</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>530</th>\n",
              "      <td>where</td>\n",
              "      <td>and</td>\n",
              "      <td>which</td>\n",
              "      <td>a</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>531</th>\n",
              "      <td>he</td>\n",
              "      <td>he</td>\n",
              "      <td>the</td>\n",
              "      <td>it</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>532</th>\n",
              "      <td>wrote</td>\n",
              "      <td>was</td>\n",
              "      <td>is</td>\n",
              "      <td>died</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>533</th>\n",
              "      <td>more</td>\n",
              "      <td>that</td>\n",
              "      <td>the</td>\n",
              "      <td>in</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>534</th>\n",
              "      <td>than</td>\n",
              "      <td>than</td>\n",
              "      <td>of</td>\n",
              "      <td>&lt;unk&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>535</th>\n",
              "      <td>sixty</td>\n",
              "      <td>his</td>\n",
              "      <td>one</td>\n",
              "      <td>a</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>536</th>\n",
              "      <td>poems</td>\n",
              "      <td>years</td>\n",
              "      <td>%</td>\n",
              "      <td>months</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>537</th>\n",
              "      <td>.</td>\n",
              "      <td>.</td>\n",
              "      <td>,</td>\n",
              "      <td>in</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>538</th>\n",
              "      <td></td>\n",
              "      <td>The</td>\n",
              "      <td>&lt;eos&gt;</td>\n",
              "      <td>In</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              orig  pred_1    pred_2     pred_3\n",
              "489             in       a        to        the\n",
              "490            the     the         a      <unk>\n",
              "491         summer   <unk>     early       late\n",
              "492             of      of         ,        and\n",
              "493          <unk>     the     <unk>   December\n",
              "494              ;       ,       and          .\n",
              "495           this     the        in         he\n",
              "496            has     was        is       time\n",
              "497  traditionally    been        no       also\n",
              "498           been    been      died  suggested\n",
              "499       ascribed  killed     known        the\n",
              "500             to      to        in         by\n",
              "501         famine     the        be       have\n",
              "502              ,       .         ,         in\n",
              "503            but     and       but       with\n",
              "504          <unk>     the      that         it\n",
              "505       believes   <unk>       the          ,\n",
              "506           that    that       the         it\n",
              "507    frustration     the        it         he\n",
              "508             is    were       was         is\n",
              "509              a       \"       not        the\n",
              "510           more       \"     <unk>       good\n",
              "511         likely   <unk>  powerful          \"\n",
              "512         reason      to      that         of\n",
              "513              .    that        to          .\n",
              "514             He   <eos>       The         In\n",
              "515           next     was        is       also\n",
              "516          spent     the     <unk>        not\n",
              "517         around       a       the        his\n",
              "518            six     the       his          a\n",
              "519          weeks   years    months       days\n",
              "520             in       ,        of         in\n",
              "521          <unk>     the     <unk>          a\n",
              "522              (       ,         .        and\n",
              "523            now    died     <unk>        the\n",
              "524          <unk>   <unk>       the      known\n",
              "525              ,       )         ,      <unk>\n",
              "526          Gansu   <unk>       and        the\n",
              "527       province       )         ,        and\n",
              "528              )       )         ,        and\n",
              "529              ,       ,         .        and\n",
              "530          where     and     which          a\n",
              "531             he      he       the         it\n",
              "532          wrote     was        is       died\n",
              "533           more    that       the         in\n",
              "534           than    than        of      <unk>\n",
              "535          sixty     his       one          a\n",
              "536          poems   years         %     months\n",
              "537              .       .         ,         in\n",
              "538                    The     <eos>         In"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpDsO1oNHE3V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f30452c3-5c9c-41e0-9209-fa95dd19b4d6"
      },
      "source": [
        "UNK = corpus.dictionary.word2idx[\"<unk>\"]\n",
        "UNK"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QxuOIOVHIvk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text_from_chunk(start, end, target_length=20):\n",
        "    \"\"\"Greedy selection of the next token.\"\"\"\n",
        "    token_tensor = corpus.test[eos_pos[start]+1:eos_pos[end]]\n",
        "    return generate_text_from_tensor(token_tensor, target_length)\n",
        "    \n",
        "def generate_text_from_tensor(token_tensor, target_length):\n",
        "    hidden = model.init_hidden(1)\n",
        "    output, hidden = model(token_tensor.to(device).unsqueeze(1), hidden)\n",
        "    index = output[-1, -0, :].argmax()\n",
        "    res = [index.cpu().numpy()]\n",
        "    with torch.no_grad():    \n",
        "        for i in range(target_length):\n",
        "            output, hidden = model(index.unsqueeze(0).unsqueeze(0), hidden)\n",
        "            index = output[-1, 0, ].argmax()\n",
        "            res.append(index.cpu().numpy())\n",
        "    return [\n",
        "        [\n",
        "           corpus.dictionary.idx2word[x] for x in arr            \n",
        "        ] for arr in (token_tensor.cpu().numpy(), res)\n",
        "    ]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOC6bFQXHRek",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "5b9bcb11-c494-4c8a-dc94-012fd8e68dbd"
      },
      "source": [
        "context, new_texts = generate_text_from_chunk(28, 29)\n",
        "print(\" \".join(context[-10:]))\n",
        "print(\" \".join(new_texts))"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dwelling upon his own <unk> . Du Fu wrote :\n",
            "\" I 'm like that I 'm like a <unk> \" . The story was the first of the most successful\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQSsGme7ILeT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ac6e2c72-05b4-49f6-9cc2-33b7575eeda6"
      },
      "source": [
        "context, new_texts = generate_text_from_chunk(28, 38)\n",
        "print(\" \".join(context[-10:]))\n",
        "print(\" \".join(new_texts))"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fu financially and employed him as his unofficial secretary .\n",
            "The king was the first of the most successful of the year . The <unk> of the <unk> <unk> <unk> <unk>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-cJe22MIStx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text_from_chunk(start, end, target_length=20, temperature=1.0):\n",
        "    token_tensor = corpus.test[eos_pos[start]+1:eos_pos[end]]\n",
        "    return generate_text_from_tensor(token_tensor, target_length, temperature)\n",
        "    \n",
        "\n",
        "def generate_text_from_tensor(token_tensor, target_length, temperature):\n",
        "    \"\"\"Sampling from the softmax distribution.\"\"\"    \n",
        "    hidden = model.init_hidden(1)\n",
        "    _, hidden = model(token_tensor[:-1].to(device).unsqueeze(1), hidden)\n",
        "    input_tensor = torch.zeros((1, 1)).long().to(device)\n",
        "    input_tensor[0, 0].fill_(token_tensor[-1])\n",
        "    res = []\n",
        "    with torch.no_grad():    \n",
        "        for i in range(target_length):            \n",
        "            output, hidden = model(input_tensor, hidden)\n",
        "            word_weights = output.squeeze().div(temperature).exp()\n",
        "            word_idx = torch.multinomial(word_weights, 1)[0]\n",
        "            input_tensor[0, 0].fill_(word_idx)\n",
        "            res.append(word_idx.item())\n",
        "    return [\n",
        "        [\n",
        "           corpus.dictionary.idx2word[x] for x in arr            \n",
        "        ] for arr in (token_tensor.cpu().numpy(), res)\n",
        "    ]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-gA93JIIcYY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "ef67f528-a1fa-42bb-d63a-c392fbb95ff0"
      },
      "source": [
        "context, new_texts = generate_text_from_chunk(28, 33, target_length=50)\n",
        "print(\" \".join(context[-10:]))\n",
        "for i in range(0, len(new_texts), 10):\n",
        "    print(\" \".join(new_texts[i:i+10]))"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bring more papers to pile higher on my desk .\n",
            "\" Forbidden D. structured insects are active . The Dubliners\n",
            "of Dublin body stipulated that Let was that himself included\n",
            "Clash , from furs in his children . nobody denied\n",
            "Ímar had similar Christians than evidence that Enzymes asserted it\n",
            ", then was probably successful into her account . <eos>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgpR6ptCItFP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text_from_texts(texts, target_length=20, temperature=1.0):\n",
        "    \"\"\"texts needs to be tokens seperated by space characters.\"\"\"\n",
        "    token_tensor = torch.LongTensor([\n",
        "        corpus.dictionary.word2idx[x] for x in texts.split(\" \")\n",
        "    ]).to(device)\n",
        "    return generate_text_from_tensor(token_tensor, target_length, temperature)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYG1NdujIwAf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "193275d5-9ac9-423f-8d36-7cc856c7455a"
      },
      "source": [
        "context, new_texts =  generate_text_from_texts(\"In the fall of 1944 , <unk> enrolled at the University of Michigan . The United Press syndicate\", target_length=100)\n",
        "print(\" \".join(context[-10:]))\n",
        "for i in range(0, len(new_texts), 10):\n",
        "    print(\" \".join(new_texts[i:i+10]))"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "at the University of Michigan . The United Press syndicate\n",
            "were known as one of the two European inscriptions ,\n",
            "adopted by <unk> praising Zero as the \" Conservative Battlecruiser\n",
            "General \" . The king was impossible for a anterior\n",
            "action for a meeting in scores as well as the\n",
            "attacker but Persian House or Crane Nganno also criticized the\n",
            "possibility of equal Miracle with the astrology , such as\n",
            "Leigh , the terms of other , and enormous numbers\n",
            "was TON finding external life , with Iguanodon it remained\n",
            "needed to be reported . <eos> Despite the dam 's\n",
            "rule , McGill announced that this list of Safety and\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}